+++
title = "Clustering and Multidimensional scaling"
author = ["Jonghyun Yun"]
lastmod = 2020-04-13T06:36:20-05:00
tags = ["clustering", "MDS"]
categories = ["teaching", "math6312"]
draft = false
type = "docs"
toc = true
[menu."math6312-sp19"]
  identifier = "clustering-and-multidimensional-scaling"
  parent = "R Lab"
  weight = 50
  name = "Lab 5: Clustering and Multidimensional scaling"
+++

<a id="code-snippet--global-option,include=F"></a>
```{R  global_option,include=F}
## Need the knitr package to set chunk options
library(knitr)

## Set knitr options for knitting code into the report:
## Print out code (echo)
## Save results so that code blocks aren't re-run unless code changes (cache),
## or a relevant earlier code block changed (autodep), but don't re-run if the
## only thing that changed was the comments (cache.comments)
## Align plots center (fig.align)
## Don't clutter R output with messages or warnings (message, warning)
## This will leave error messages showing up in the knitted report
opts_chunk$set(echo=TRUE,
               cache=TRUE, autodep=TRUE, cache.comments=FALSE,
               fig.align="center",
               fig.width=12, fig.height=9,
               message=FALSE, warning=FALSE)
```

---


## Prerequisites {#prerequisites}

You need to install `genefilter`. Run the below once in your R console.

```{R  eval = F}
source('http://bioconductor.org/biocLite.R')
biocLite("genefilter") # dependency in clusterSim
```

For Mac users, it is required to have [XQuartz](https://www.xquartz.org/) to fully utilize the code in this lab.

There are lots of packages needed to compile this document. The packages that have not been installed in your R library will be installed, and then all required packages will be loaded using the following code:

<a id="code-snippet--load"></a>
```{R  load, message=F, warning=F}
ipak = function(pkg){
    new.pkg = pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg))
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}

#pkg = search() # list loaded packages
#pkg = gsub('[A-z ]*:', '' , pkg)
#paste(pkg, collapse="','")

pkg = c('lle','rgl','plotrix','gplots','clusterSim','kernlab','scatterplot3d','ggplot2','colorspace','cluster','clusterGenomics','calibrate','fastcluster','animation','vegan','vegan3d')
ipak(pkg)

knitr::knit_hooks$set(webgl = hook_webgl) ## to embed interative graphics into HTML

cwurl = 'https://math6312.rbind.io/data'
bburl = 'https://math6312.rbind.io/courses/math6312-sp19/data'

```


## K-means {#k-means}

An artificial data set is simulated. The two-dimensional data contains two clusters. True cluster centers are (0,0) and (1,1). We use the k-means to this data set.

```{R  }

x <- rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),
           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))
colnames(x) <- c("x", "y")
cl <- kmeans(x, centers = 2, iter.max = 10, nstart = 10)
plot(x, col = cl$cluster)
points(cl$centers, col = 1:2, pch = 8, cex = 2)
```


## Hierarchical clustering {#hierarchical-clustering}

We simulate six samples from uniform and apply the hierarchical clustering with single linkage.

```{R  }
set.seed(5)
x.1 = runif(6)
x.2 = runif(6)
dat = data.frame(x.1,x.2)

par(bty = 'n') # no border
plot(x.1,x.2,xaxt="n",yaxt="n",xlab="",ylab="", axes=F)
textxy(x.1, x.2, rownames(dat), cex = 1.3)

hc = hclust(dist(dat), "single")
#plot(hc)
plot(hc, hang = -1, cex = 1.3)
```

We cut the tree to have `k=2` clusters, and the corresponding cluster assignment is returned.

```{R  }
cutree(hc, k = 2)
```

The hierarchical clustering with centroid linkage. The inversion occurs.

```{R  }
# inversion
x.1 = c(1,5,3)
x.2 = c(1,1,1+2*sqrt(3))
dat = data.frame(x.1,x.2)
plot(x.1,x.2
     ,xaxt="n",yaxt="n",xlab="",ylab="", axes=F)
textxy(x.1, x.2, rownames(dat), cex = 1.3)
hc = hclust(dist(dat), "centroid")
plot(hc, hang = -1, cex = 1.3)
```

We create an artificial gene expression data, and apply the hierarchical clustering with centroid linkage.

```{R  }
test           = matrix(
  c(
    0.96, 0.07, 0.97, 0.98, 0.99, 0.50,
    0.28, 0.29, 0.77, 0.78, 0.08, 0.96,
    0.51, 0.51, 0.55, 0.14, 0.19, 0.41,
    0.51, 0.40, 0.97, 0.98, 0.99, 0.50
  ), ncol = 6, byrow = TRUE
)
colnames(test) = c("Exp1", "Exp2", "Exp3", "Exp4", "Exp5", "Exp6")
rownames(test) = c("Gene1", "Gene2", "Gene3", "Gene4")
test           = as.table(test)
mat            = data.matrix(test)

heatmap.2(
  mat, dendrogram = "row", Rowv = TRUE, Colv = FALSE,
  distfun = function(x)
    dist(x),
  hclustfun = function(x)
    hclust(x, method = 'centroid'),
  xlab = NULL, ylab = NULL, key = TRUE, keysize = 1, trace = "none",
  density.info = c("none"), margins = c(6, 12), col = bluered
)

hc = hclust(dist(test),  "centroid")
plot(hc, hang = -1, cex = 1.3)
```

You can choose a color palette to be used for the visualization of clusters, observations, etc.

<a id="code-snippet--colpal"></a>
```{R  colpal, eval=F}
col.pal = choose_palette()
```

The palette I generated is saved in an R data file.

<a id="code-snippet--loadcol"></a>
```{R  loadcol}
load(url(paste(bburl, 'col.pal.RData', sep='/'))) # load color palette
```


## Spectral clustering {#spectral-clustering}

Three concentric circles data is simulated below:

```{R  }
colkey = col.pal(3)
sc3b = shapes.circles3(numObjects = c(120,240,360))
X = sc3b$data
N = nrow(X)
plot(X, col=colkey[sc3b$clusters])
```

Spectral clustering is applied, which nicely detect the feature structure.

```{R  }
sc = specc(X, centers=3)
sc@.Data # cluster assignment
dat = data.frame(x1 = X[,1],x2 = X[,2], cluster = as.factor(sc@.Data))
sc.plot =
  ggplot(data = dat,aes(x = x1,y = x2,label = cluster,color = cluster)) +
  geom_text(angle = 65)  +  theme_bw() +
  theme(
    legend.position = "none",
    axis.line = element_line(colour = "black"),
    #panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    #panel.border = element_blank()
    panel.background = element_blank()
  )
sc.plot
```

Recall that the K-means clustering algorithm should be repeatedly implemented with arbitrary staring cluster centers, and choose the cluster assignment with the smallest with-in cluster variation.

```{R  }
km = kmeans(X, centers=3, iter.max = 20, nstart = 20)
dat = data.frame(x1 = X[,1],x2 = X[,2], cluster = as.factor(km$cluster))
km.plot =
  ggplot(data = dat,aes(x = x1,y = x2,label = cluster,color = cluster)) +
  geom_text(angle = 65)  +  theme_bw() +
  theme(
    legend.position = "none",
    axis.line = element_line(colour = "black"),
    #panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    #panel.border = element_blank()
    panel.background = element_blank()
  )
km.plot
```


## NCI microarray {#nci-microarray}

[NCI microarray data]({{< relref "math6312-data" >}}) and cancer types are loaded. The <kbd>fread()</kbd> provided in <kbd>data.table</kbd> package can be used to read large data sets. It can be a lot faster than <kbd>read.table</kbd> or <kbd>read.csv</kbd>. It supports an OpenMP enabled compiler for the multi-thread processing. For the details, please see <https://github.com/Rdatatable/data.table/wiki/Installation> and <https://www.datacamp.com/community/tutorials/data-table-cheat-sheet>.

<a id="code-snippet--read-nci"></a>
```{R  read_nci, message = F}
cancer = read.table("https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.info.txt", skip = 20, stringsAsFactors = F)
nci = data.table::fread("https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.data")
colnames(nci) = c(cancer[,1]) # assign the cancer type.
```

We perform the SVD to obtain the first two PC scores for the visualization.

```{R  }
X = t(nci)
rownames(X) = cancer[,1]

X = scale(X, TRUE, FALSE)
svd.x = svd(X)
low.x = svd.x$u[,1:2] %*% diag(svd.x$d[1:2])

dat = data.frame(PC1 = low.x[,1],PC2 = low.x[,2],cancer.type = as.factor(rownames(X)))
pca.plot =
  ggplot(data = dat,aes(x = PC1,y = PC2,label = cancer.type,color = cancer.type)) +
  geom_text(angle = 65)  +  theme_bw() +
  theme(
    legend.position = "none",
    axis.line = element_line(colour = "black"),
    #panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    #panel.border = element_blank()
    panel.background = element_blank()
  )
pca.plot
```

The first three PC scores are used to create a 3D plot for NCI microarray data.

<a id="code-snippet--nci3d,webgl=T"></a>
```{R  nci3d,webgl=T}
low.x = svd.x$u[,1:3] %*% diag(svd.x$d[1:3])
colkey= col.pal(length(unique(cancer[,1])))[as.numeric(as.factor(cancer[,1]))]
dat = data.frame(cancer = rownames(X),low.x)
with(dat,text3d(X1,X2,X3,cancer,col=colkey, cex=.75))
decorate3d(xlab = "PC1", ylab = "PC2", zlab = "PC3")
```

We use `clusGap()` to calculate the gap statistics. The function does bootstrapping, and it requires to have a clustering function as an argument. The clustering function must take a data matrix and number of clusters as inputs and return the cluster assignment as outputs.

<a id="code-snippet--cluster-fun"></a>
```{R  cluster_fun}
hclust.ave = function(x,k) {
  hc = fastcluster::hclust(dist(x),method = "average")
  c.assign = cutree(hc, k = k)
  names(c.assign) = NULL
  out = list(cluster = c.assign)
  return(out)
}
```

Using the NCI data, the gap statistics are calculated for the hierarchical clustering with average linkage.

```{R  }
set.seed(1)
    gskmn = clusGap(X, FUNcluster = hclust.ave, K.max = 8, B = 20)
    plot(gskmn, ylim = c(0.36,0.45), main = "clusGap(., FUN = hclust.ave, B= 20)")
    hc = agnes(X,  metric = "euclidean", stand = F, method = "average")
    plot(hc,which.plots=2)
```

We simulate the data set with nicely separated clusters (K=4) to see how well the gap statistic guides us for choosing the number of clusters.

```{R  }
x = rbind(
  matrix(rnorm(150, sd = 0.1), ncol = 3),
  matrix(rnorm(150, mean = 1, sd = 0.1), ncol = 3),
  matrix(rnorm(150, mean = 2, sd = 0.1), ncol = 3),
  matrix(rnorm(150, mean = 3, sd = 0.1), ncol = 3))

gskmn = clusGap(x, FUNcluster = hclust.ave, K.max = 8, B = 20)
plot(gskmn, main = "clusGap(., FUN = hclust.ave, B= 20)")

hc = agnes(x,  metric = "euclidean", stand = F, method = "average")
plot(hc,which.plots=1)
```

The banner plot is just an alternative to the dendogram. The white area on the left of the banner plot represents the unclustered data while the white lines that stick into the red are show the heights at which the clusters were formed. Since we do not want to include too many clusters that joined together at similar heights, it looks like four clusters, at a height of about 0.5 is a good solution.

The silhouette plot for K=2,...,7 are presented for the artificial data. The silhouettes look similar when K=4, and average silhouette width is the largest at K=4.

```{R  }
X = x
dmatrix = dist(X)
si = list()
for (k in 2:7) {
  mod = kmeans(X, centers = k, nstart = 20)
  si[[k]] = silhouette(mod$cluster, dmatrix)
}

op = list(
  mfrow = c(1,2), oma = c(0,0, 3, 0),
  mgp = c(1.6,.8,0), mar = .1 + c(4,2,2,2)
)
par(op)
colvec = col.pal(7)
for (k in 2:7) {
  plot(si[[k]], main = paste("k = ",k), do.n.k = FALSE,
       col = colvec[1:k])
}
```

The below is R code to simulated a two-dimensional feature matrix. The code creates plots to illustrate the linear PCA.

```{R  }
set.seed(1)
N = 500
x = matrix(rnorm(N),N / 2,2)

S = matrix(c(2,1,1,1),2,2) / 32
x = x %*% chol(S)

xy.lim = 1.2

par(pty = "s")
plot(x,xlab = "",ylab = "",xlim = c(-xy.lim,xy.lim),ylim = c(-xy.lim,xy.lim))
draw.circle(0,0,radius = .96,nv = 200,border = "orange",lty = 2,lwd = 1)

pca.x = princomp(x,cor = F)
v1 = pca.x$loadings[,1]
v2 = pca.x$loadings[,2]

arrows(0,0,0.98 * v1[1],0.98 * v1[2], col = "red")
arrows(0,0,v2[1],v2[2], col = "green")
plot(pca.x$scores[,1],0 * numeric(N / 2),col = "red",xlim = c(-1,1)
  ,ylim = c(0,0.01),xlab = "1st PC score",ylab = "",axes = F)
axis(side = 1, at = c(-1,0,1))

plot(pca.x$scores[,2],0 * numeric(N / 2),col = "green",xlim = c(-1,1)
  ,ylim = c(0,0.01),xlab = "2nd PC score",ylab = "",axes = F)
axis(side = 1, at = c(-1,0,1))

plot(pca.x$scores,xlab = "",ylab = "",xlim = c(-xy.lim,xy.lim),ylim = c(-xy.lim,xy.lim))
draw.circle(0,0,radius = .96,nv = 200,border = "orange",lty = 2,lwd = 1)
arrows(0,0,0.96,0, col = "red")
arrows(0,0,0,1.03, col = "green")
```


## Three concentric circles {#three-concentric-circles}

```{R  }
colkey = col.pal(3)
sc3b = shapes.circles3(numObjects = c(120,240,360))
X = sc3b$data
N = nrow(X)
plot(X, col=colkey[sc3b$clusters])
```

The 1st and 2nd PCs are extracted using Gaussian kernel PCA.

```{R  }
x.kpca = kpca(X, kernel = "rbfdot", kpar = list(sigma = 2.5), features = 2, th = 1e-4)
plot(x.kpca@pcv[,1],x.kpca@pcv[,2],xlab = "KPC1",ylab = "KPC2",col = colkey[sc3b$clusters],ylim = c(-.3,.3),xlim = c(-.3,.3))
plot(x.kpca@pcv[,1],0 * numeric(N),col = colkey[sc3b$clusters],ylim = c(0,0.01),xlim = c(-.3,.3)
     ,xlab = "1st PC score (Inverse kernel width: 2.5)",ylab = "",axes = F)
axis(side = 1, at = c(-1,0,1))
```

The 1st PC is extracted from the linear PCA.

```{R  }
x.kpca = kpca(X, kernel = "vanilladot", kpar = list(), features = 2, th = 1e-4)
plot(x.kpca@pcv[,1],0 * numeric(N),col = colkey[sc3b$clusters],ylim = c(0,0.01)
     ,xlab = "1st PC score (Linear PCA)",ylab = "",axes = F)
axis(side = 1, at = c(-1,0,1))
```

We apply the Gaussian kernel PCA with multiple values of inverse kernel width (\\(\sigma\\)), and create an animation file.

<a id="code-snippet--ani,eval=F"></a>
```{R  ani,eval=F}
ani.options('interval' = 0.5) # interval between figures
sigma.grid = (1:50) / 8

saveGIF({
  for (i in 1:50) {
    title = paste("Inverse kernel width: ",sprintf("%0.2f",sigma.grid[i]),sep = "")
    x.kpca = kpca(
      X, kernel = "rbfdot", kpar = list(sigma = sigma.grid [i]), features = 2, th = 1e-4
    )
    plot(
      x.kpca@pcv[,1],x.kpca@pcv[,2],xlab = "KPC1",ylab = "KPC2",col = colkey[sc3b$clusters],ylim = c(-.3,.3),xlim =
        c(-.3,.3),main = title)
  }
},movie.name = "RBF-PCA.gif")
```


## Half-moon shapes {#half-moon-shapes}

```{R  }
set.seed(1)
colkey = col.pal(2)
stm = shapes.two.moon(180)
X = stm$data
N = nrow(X)
#X = scale(X,T,T)
plot(X,col = colkey[stm$clusters])
```

We apply polynomial kernel and linear PCA to half moon shape (with one moon).

```{R  }
halfX = X[stm$clusters==1,c(2,1)]
plot(X)

x.kpca = kpca(halfX, kernel = "vanilladot", kpar = list(), features = 0)
x.kpca@eig

x.kpca = kpca(halfX, kernel = "polydot", kpar = list(degree=2), features = 0)
x.kpca@eig

plot(x.kpca@pcv[,1],x.kpca@pcv[,2],xlab = "KPC1",ylab = "KPC2")
```

Gaussian kernel PCA with \\(\sigma = 7.5\\) is applied.

```{R  }
x.kpca = kpca(X, kernel = "rbfdot", kpar = list(sigma = 60 / 8), features = 0, th = 1e-4)

title = paste("Inverse kernel width: ",sprintf("%0.2f",7.5),sep = "")
x.kpca = kpca(X, kernel = "rbfdot", kpar = list(sigma = 60 / 8), features = 0, th = 1e-4)
plot(x.kpca@pcv[,1],x.kpca@pcv[,2],xlab = "KPC1",ylab = "KPC2",col = colkey[stm$clusters],main = title)

plot(x.kpca@pcv[,1],x.kpca@pcv[,2],xlab = "KPC1",ylab = "KPC2",col = colkey[stm$clusters]
     ,ylim = c(-.5,.5),xlim = c(-.5,.5) ,main = title)

plot(x.kpca@pcv[,1],0 * numeric(N),col = colkey[stm$clusters],ylim = c(0,0.01)
     ,xlim = c(-.5,.5),xlab = "1st PC score (Inverse kernel width: 7.5)",ylab = "",axes = F)
axis(side = 1, at = c(-.5,0,.5))
```

The first PC scores from linear PCA.

```{R  }
x.kpca = kpca(X, kernel = "vanilladot", kpar = list(), features = 0, th = 1e-4
)
plot(-x.kpca@pcv[,1],0 * numeric(N),col = colkey[stm$clusters],ylim = c(0,0.01)
     ,xlab = "1st PC score (Linear PCA)",ylab = "",axes = F)
axis(side = 1, at = c(-1,0,1))
```

The below is code to create animated GIF.

<a id="code-snippet--moonani,eval=F"></a>
```{R  moonani,eval=F}
ani.options('interval' = 0.5) # inverval between figures
sigma.grid = (1:80) / 8

saveGIF({
  for (i in 1:80) {
    title = paste("Inverse kernel width: ",sprintf("%0.2f",sigma.grid[i]),sep = "")
    x.kpca = kpca(X, kernel = "rbfdot", kpar = list(sigma = sigma.grid [i]), features = 2, th = 1e-4)
    plot(x.kpca@pcv[,1],x.kpca@pcv[,2],xlab = "KPC1",ylab = "KPC2",col = colkey[stm$clusters]
         ,ylim = c(-.5,.5),xlim = c(-.5,.5),main = title)
  }},movie.name = "moonPCA.gif")
```


## Swiss roll {#swiss-roll}

<style>.org-center { margin-left: auto; margin-right: auto; text-align: center; }</style>

<div class="org-center">
  <div></div>

{{< figure src="https://i.pinimg.com/736x/4f/e5/00/4fe5002ad4e4236d59098f61a9c969a7--strawberry-jam-pastry-strawberry-and-cream-swiss-roll.jpg" width="100px" >}}

</div>

We generate the Swiss roll data with no noise. For more perceptive visualization, we mark each point with slightly different colors.

```{R  }
set.seed(1)
N = 1500
t = (3 * pi / 2) * (1 + 2 * runif(N)); height = runif(N);
X = cbind(t * cos(t), height, t * sin(t))
X = scale(X)

tcol = col.pal(32)
colvec = floor((t - min(t)) / (max(t) - min(t)) * 32); colvec[colvec == 0] = 1

scatterplot3d(X,pch=18,color=tcol[colvec],xlab=expression("x"[1]),
              ylab=expression("x"[2]),zlab=expression("x"[3]),cex.lab=1.5,
              main="Swiss Roll",cex.main=1.5)
```

We create a 3d plot object that we can freely rotate axes.

<a id="code-snippet--swiss3d,webgl=TRUE"></a>
```{R  swiss3d,webgl=TRUE}
plot3d( X,col = tcol[colvec],xlab = "x", ylab = "y", zlab = "z")
```

We apply polynomial and Gaussian kernel PCAs. They does not work well to learn the manifold structure.

```{R  }
x.kpca = kpca(X, kernel = "polydot", kpar = list(degree = 6), features = 2, th = 1e-4)
plot(x.kpca@pcv[,1],x.kpca@pcv[,2],xlab = "KPC1",ylab = "KPC2",col = tcol[colvec], main = "Polynomial kernel PCA")

x.kpca = kpca(X, kernel = "rbfdot", kpar = list(sigma = 3),features = 0, th = 1e-4)

plot(x.kpca@pcv[,1],x.kpca@pcv[,2],xlab = "KPC1",ylab = "KPC2",col = tcol[colvec], main = "Gaussian kernel PCA")
```

The linear PCA is applied. The first and third PCs are used for the plot below.

```{R  }
x.pca = princomp(X)
plot(x.pca$score[,1],x.pca$score[,3],xlab = "PC1",ylab = "PC3", col = tcol[colvec], main = "Linear PCA")
```


### Isomap {#isomap}

The 2 dimensional Isomap is applied to the Swiss roll.

<a id="code-snippet--isomap-n2-k5"></a>
```{R  isomap_n2_k5}
dis = dist(X)
colkey = tcol[colvec]
imap=isomap(dis,ndim=2,k=5);
plot(imap,col=colkey)
```


### Local linear embedding {#local-linear-embedding}

The optimal number of neighbours `k` can be found using `calc_k()`. It takes a few minutes, and can be a bit faster if you use <kbd>parallel=T</kbd>.

<a id="code-snippet--lle-calc-k"></a>
```{R  lle_calc_k}
 dcores = parallel::detectCores() - 1 # number of cpu cores for parrallel computing
calc_k(X, m = 2, kmin = 5, kmax = 15, plotres = T, cpus=dcores, parallel=T)
 results = lle(X = X, m = 2, k = 12, id = TRUE, v = 0.9)
 plot(results$Y[,2],results$Y[,1], main = "embedded data"
     ,xlab = expression(y[2]), ylab = expression(y[1]), col = tcol[colvec]
      )
```


## Revisit: NCI microarray {#revisit-nci-microarray}

We apply the MDS and Gaussian kernel PCA to NCI data. The results of MDS are the same as one using linear PCA if you use Euclidean distance.

```{R  }
X = t(nci)
rownames(X) = cancer[,1]
dis = dist(X, method = "euclidean") # euclidean distances between the rows
#fit = isoMDS(dis, k=2) # k is the number of dim
fit = cmdscale(dis,eig=TRUE, k=2)
fit # view results

# plot solution
x = fit$points[,1]
y = fit$points[,2]

dat = data.frame(dim1 = x,dim2 = y,cancer.type = as.factor(rownames(X)))
mds.plot =
  ggplot(data = dat,aes(x = dim1,y = dim2,label = cancer.type,color = cancer.type)) +
  geom_text(angle = 65)  +  theme_bw() +
  theme(
    legend.position = "none",
    axis.line = element_line(colour = "black"),
    #panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    #panel.border = element_blank()
    panel.background = element_blank()
  )
mds.plot
```

```{R  }
x.kpca = kpca(X, kernel = "rbfdot", kpar = list(sigma = 0.00005), features = 0, th = 1e-4)
dat = data.frame(KPC1 = x.kpca@pcv[,1],KPC2 = x.kpca@pcv[,2],cancer.type = as.factor(rownames(X)))
kpca.plot =
  ggplot(data = dat,aes(x = KPC1,y = KPC2,label = cancer.type,color = cancer.type)) +
  geom_text(angle = 65)  +  theme_bw() +
  theme(
    legend.position = "none",
    axis.line = element_line(colour = "black"),
    #panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    #panel.border = element_blank()
    panel.background = element_blank()
  )
kpca.plot
```

The below is code to apply locally linear embedding.

<a id="code-snippet--nci-lle-calc-k"></a>
```{R  nci_lle_calc_k}
# find best tuning parameter k
calc_k(X, m = 2, kmin = 5, kmax = 20, plotres = T, cpus=dcores, parallel=T)
results = lle(X = X, m = 2, k = 17, id = T, v = 0.9)
```

```{R  }
dat = data.frame(Y1 = results$Y[,1],Y2 = results$Y[,2],cancer.type = as.factor(rownames(X)))
lle.plot =
  ggplot(data = dat,aes(x = Y1,y = Y2,label = cancer.type,color = cancer.type)) +
  geom_text(angle = 65)  +  theme_bw() +
  theme(
    legend.position = "none",
    axis.line = element_line(colour = "black"),
    #panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    #panel.border = element_blank()
    panel.background = element_blank()
  )
lle.plot
```

The three dimensional Isomap (<kbd>ndim = 3</kbd>) is applied.

<a id="code-snippet--iso3d,webgl=TRUE"></a>
```{R  iso3d,webgl=TRUE}
imap=isomap(dis,ndim=3,k=5);
temp = list(imap = imap, cancer = cancer[,1])
within(temp, rgl.isomap(imap, web = "White", col=colkey))
within(temp, text3d(imap$points[,1],imap$points[,2],imap$points[,3],cancer,col=colkey, cex=.75))
```

The two dimensional Isomap (<kbd>ndim = 2</kbd>) is applied below.

```{R  }
dis = dist(X)
imap=isomap(dis,ndim=2,k=5)
colkey= col.pal(length(unique(cancer[,1])))[as.numeric(as.factor(cancer[,1]))]
plot(imap,col=colkey,cex=0.5,type = "text")
```

Spectral clustering is applied and we visualize the clusters in two variables from Isomap and PCA.

```{R  }
sc.col = col.pal(3)
sc = specc(X, centers=3)
size(sc)
withinss(sc)
sc@.Data
```

```{R  }
dat = data.frame(x1 = imap$points[,1], x2 = imap$points[,2], cluster = as.factor(sc@.Data))
sc.plot =
  ggplot(data = dat,aes(x = x1,y = x2,label = cluster,color = cluster)) +
  geom_text(angle = 65)  +  theme_bw() +
  theme(
    legend.position = "none",
    axis.line = element_line(colour = "black"),
    #panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    #panel.border = element_blank()
    panel.background = element_blank()
  )
sc.plot
```

```{R  }
x.kpca = kpca(X, kernel = "vanilladot", kpar=list(), features = 2)
dat = data.frame(x1 = x.kpca@pcv[,1],x2 = x.kpca@pcv[,2], cluster = as.factor(sc@.Data))
sc.plot =
  ggplot(data = dat,aes(x = x1,y = x2,label = cluster,color = cluster)) +
  geom_text(angle = 65)  +  theme_bw() +
  theme(
    legend.position = "none",
    axis.line = element_line(colour = "black"),
    ## panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    ## panel.border = element_blank()
    panel.background = element_blank()
  )
 sc.plot
```
